,tweet,conj+,conj&,names?,Notes
1,.@uwnlp researchers Trang Tran & Mari Ostendorf on identifying sub-reddit style (top) and topic (bottom) #emnlp2016 https://t.co/C7wYIvB2rK,,,y,
2,Tabassum et al:  our system is the first temporal tagger designed for social media data & it doesn't require hand tagging #emnlp2016,,our system is the first temporal tagger designed for social media data & it doesn't require hand tagging ,,
3,Upadhyay et al: our data is avaliable = https://t.co/5vOmqTtD5r & https://t.co/uzcVFIlXWf #emnlp2016,, https://t.co/5vOmqTtD5r & https://t.co/uzcVFIlXWf ,,
4,Gorman & Sproat: Q- what about Roman numerals? A- Sure! That's actually in our system already. #emnlp2016,,,y,
5,"Gorman & Sproat: takeaway = if you want to efficiently use small datasets, you can draw on the linguistics literature to do this  #emnlp2016",,,y,
6,"Gorman & Sproat: we get almost perfect performance on a range of language (English, Georgian, Russian) and no silly errors #emnlp2016",,,y,
7,Gorman & Sproat: our number grammars use both language universal and language specific components #emnlp2016,,,y,
8,Gorman & Sproat: We found linguistic research on this! Hurford (1975) described number grammars as context-free grammars (CFG) #emnlp2016,,,y,
9,"Gorman & Sproat: For handwriting grammars you could use Wikipedia for data, don't need to know the language #emnlp2016",,,y,
10,"Gorman & Sproat: What about a hand-written finite state transducer? Need to deal with things like morphology, phonology, etc. #emnlp2016",,,y,
11,"Gorman & Sproat: LSTM & attention models are basically perfect on medium and large dataset, but learn close to nothing with small #emnlp2016",, LSTM & attention models,y,
12,"Gorman & Sproat: training data is large, med and small datasets from Russian (which is the hardest language we work on for this) #emnlp2016",,,y,
13,Gorman & Sproat: Looking at two types of models today: 1) machine learning model (RNNs) 2) finite state transducers  #emnlp2016,,,y,
14,"Gorman & Sproat: For new speech products, we start with a hand-written number grammar, have around 70 of these #emnlp2016",,,y,
15,"Gorman & Sproat: We use RNN's, but got ""silly errors"", like '72"" being read aloud as ""four hundred and seventy two"" were common #emnlp2016",,,y,
16,"Gorman & Sproat: Getting these right, e.g. correctly reading ""69"" as ""sixty-nine"" is really important for usable text-to-speech #emnlp2016",,,y,
17,"Gorman & Sproat: Text-to-speech requires text normalization for verbalization of a variety of special cases: currency, dates etc #emnlp2016",,,y,
18,Gorman & Sproat: [TACL paper] Minimally supervised models for number normalization #emnlp2016,,,y,
19,Marinho et al:  How do we find anchors? We use a small labeled corpus w/ small lexicon & choose words with high prob. of 1 label #emnlp2016,,use a small labeled corpus w/ small lexicon & choose words with high prob. of 1 label,y,
20,"@ErikaVaris I think it maaaayyy have something to do with length of the constituants, longer = more likely to use &",,,,
21,"I'm genuinely curious about when I use ""+"" vs ""&"" to replace ""and"". Just realized today I have strong intutions about when to use each.",,,,
22,Braud & Denis: our data + code is available = https://t.co/3jYCDGuc6A #emnlp2016,Data + code,,,
23,Braud & Denis: We used data from the Bllip dataset = https://t.co/2bZ7F3tJoG #emnlp2016,,,y,
24,"Braud & Denis: we use connectives (because, after, since) to find the relevant context to characterize relationship between words #emnlp2016",,,y,
25,"Braud & Denis: One way around is to look at features of word pairs. E.g if some prices ""rose"" and ""tumbled"", that's contrastive. #emnlp2016",,,y,
26,"Braud & Denis: It's harder to find them automatically without explicit cues, e.g. [I'm here.] [Sunday I go home.] No linking then #emnlp2016",,,y,
28,Braud & Denis: Learning connective-based word representations for implicit discourse relation identification #emnlp2016,,,y,
29,Guo et al:  the entities & relations can be embedded into a continuous vector space to simplify manipulation + preserve struture #emnlp2016,to simplify manipulation + preserve struture,the entities & relations,,
30,Lei et al: Applied to beer ratings dataset (ratings + text). Goal is to predict both ratings and correct rationale for them. #emnlp2016,Ratings + text,,,
31,@kirbyconrod full paper should be online in proceedings. Title + first author is in the first tweet of the thread.,Title + first author,,,
32,Hahn & Keller: NEAT does approximate human fixation rate based on POS categories (unlike predictions based on word frequencies) #emnlp2016,,,y,
33,"Hahn & Keller: Our model did well, but supervised feature-based models were at human-like levels in predicting word fixation.  #emnlp2016",,,y,
34,Hahn & Keller: Data from the Dundee eye tracking corpus: https://t.co/3YQF0qwaiM #emnlp2016,,,y,
35,"Hahn & Keller: our approach is NEAT (NEural Attention Tradeoff), trained model and then tested on eye tracking corpus #emnlp2016",,,y,
36,Hahn & Keller: This talk presents a model that 1) is unsupervised and 2) accounts for words that are both fixated on and skipped #emnlp2016,,,y,
37,"Hahn & Keller: Many supervised approaches to modelling this, one unsupervised model: suprisal (how unexpected a given word is) #emnlp2016",,,y,
38,"Hahn & Keller: During reading we jump from one word to another, with more attention on content than function words #emnlp2016",,,y,
39,Hahn & Keller: Modeling human reading with neural attention #emnlp2016,,,y,
40,Potts: Looked @ language + action with Cards Task corpus (https://t.co/KjF3GKNjgK) a shared map task.  #emnlp2016,Language + action,,,
41,"Potts: Today I'll be focusing on Gricean pragmatics, which is summarized by the cooperative principle + Gricean maxims.  #emnlp2016",the cooperative principle + Gricean maxims,,,
42,This year there are four Platinum #emnlp2016 sponsers: @amazon @baidu @Grammarly & @google,,@amazon @baidu @Grammarly & @google,,
43,"Pappas & Popescu-Belis: Used a crowdsourcing task, 20 k annotations of 1662 sentences. Data: https://t.co/HvkJNy55vD #emnlp2016",,,y,
44,"Pappas & Popescu-Belis: model error rate is lower than earlier work, but we needed data to describe sentence-level accuracy #emnlp2016",,,y,
45,Pappas & Popescu-Belis: our materials/code are on-line: https://t.co/nwkUow5UiD (wmil & wmil-sgd)  #emnlp2016,,,y,
46,Pappas & Popescu-Belis: used multiple-instance regression. Better suited for weak (bag-level) labels & interpretable and flexible #emnlp2016,,Better suited for weak (bag-level) labels & interpretable and flexible ,y,
47,"Pappas & Popescu-Belis: our earlier work (2014) and Yang et al (2016) incorporated attention, now improving on those models #emnlp2016",,,y,
48,"Pappas & Popescu-Belis: case study = aspect ratings of review (e.g. of a radio drama, story was good, narration was bad, etc.) #emnlp2016",,,y,
49,Pappas & Popescu-Belis: Human versus machine attention in document classification: A dataset with crowdsourced annotations #emnlp2016,,,y,
50,Jaech et al: Used two hand-labelled Twitter datasets =  TweetLID (confusable Iberian language + English) & Twitter70 #emnlp2016,confusable Iberian language + English,,,
51,Another project from @uwengineering & @uwnlp :) https://t.co/vbjBUUfHpk,, @uwengineering & @uwnlp,,
52,Hsieh et al:  Applied our model to articles about Taiwanese presidential election. Classified as + or - & then compared to polls #emnlp2016,, Classified as + or - & then compared to polls ,,plus is being used to mean positive here
53,"Fang et al: What makes something popular? Context, author rep., comment length, content. We're looking at context & content #emnlp2016",,context & content,,
54,@mareberl someone asked that during the Q&A and I genuinely can't remember what the answer was. Sorry üòì,,Q&A,,this is probably a set form
55,"Dredze et al:  this is a social media corpus designed for entity disambiguation, both entity linking + entity clustering  #emnlp2016",entity linking + entity clustering,,,
56,Doggett & Cantarero: code for our linguistic rules is available: https://t.co/ARHlkgvUel  #emnlp2016,,,y,
57,Doggett & Cantarero: Accuracy depends (not surprisingly) on whether something happened on a given day or not #emnlp2016,,,y,
58,Doggett & Cantarero: this strategy also worked really well for getting celebrity sightings #emnlp2016,,,y,
59,Doggett & Cantarero: we looked at tweets that were close to each other in time + space (using predicted geo-location) #emnlp2016, time + space,,y,
60,"Doggett & Cantarero: once we've filtered our tweets, how do we determine which relate to a specific event? #emnlp2016",,,y,
61,"Doggett & Cantarero: we also blocked some things -- jokes, wrong mood/tense/person/temporal markers, pop culture references #emnlp2016",,,y,
62,"Doggett & Cantarero: characterises of eyewitness tweets: 1st person, time markers, locatives, expletives, emotive puncuation #emnlp2016",,,y,
63,Doggett & Cantarero: We used a fairly simple system -- combining linguistic rules + statio-temporal clustering #emnlp2016,linguistic rules + statio-temporal clustering,,y,
64,"Doggett & Cantarero: Twitter regularly breaks news, but identifying important tweets is difficult #emnlp2016",,,y,
65,Doggett & Cantarero: Identifying Eyewitness News-worthy Events on Twitter (Doggett is @ErikaVaris) #emnlp2016,,,y,
66,Danescu-Niculescu-Mizil: in 30% of games the group performance was below average & in 17% group was worse than worst individual #emnlp2016,,in 30% of games the group performance was below average & in 17% group was worse than worst individual ,y,
67,Danescu-Niculescu-Mizil: We developed an online game to determine group + individual effectiveness: https://t.co/4BTKlBEbXQ #emnlp2016,group + individual ,,y,
68,"Danescu-Niculescu-Mizil: using these features & our corpus, we built a SVN classifier that predicted politeness near human levels #emnlp2016",,these features & our corpus,y,
69,@latinamnonvoco cool! I'm hyposmic so it's always interesting to learn what has smells. Earlier this year I learned about apples & concrete.,,apples & concrete,,
70,@lingulate could be pre-velar raising? @aliciabwassink & Valerie Freeman might be the ones to ask,,,y,
71,"@kleinschmidt that said, I've stuck with it b/c it makes coding and installing new packages much easier & faster (ymmv)",,easier & faster,,
72,"Me: oh, I don't have the pin/pen merger
Also me: ""windy"" and ""Wendy"" are perfect homophones
#classicPartialMerger  #alsoBeen&Ben #butNotBin?",, #alsoBeen&Ben,,
73,"@negativecos A+, maybe best selfie ever?",,,,"A+, not conj"
74,This is an A+ slide. https://t.co/p8y6x7rCEv,,,,"A+, not conj"
75,@kirbyconrod huh. I think I say /…îks/ for the cable and /…ëks/ for the syntax thing + cattle,syntax thing + cattle,,,
76,"""bonus"" /bon…™s/ --&gt; /b…î äÃÉs/ (L1 = Brazilian Portuguese)",,,,
77,".@yenerm: Java 7, 8 have many differences (static + default methods on interfaces, lambda expressions, method references) #DevFestSEA",static + default methods,,,
78,.@martinomander opening up #DevFestSea with a talk on game development + JavaScript, game development + JavaScript,,,
79,".@rikkoncelkedzio = this will be presented as a poster + long paper @emnlp2016, come check it out!",poster + long paper,,,
80,"we use a technique we call ""theme rewriting"" =  Jim + Brenda pick x apples --&gt; Leia and Han blast x droids or Finn + Jake bump x fists","Jim + Brenda, Finn + Jake",,,
81,problem: math word problems are boring & kids learn better if they're interested in the theme of the problem,,math word problems are boring & kids learn better if they're interested in the theme of the problem,,
82,"Oh, and I have a handy-dandy worksheet + tips to help you get started: https://t.co/SENDp0hTyd",handy-dandy worksheet + tips,,,
83,"Really pleased by the feedback from my @swcarpentry R students this morning: ""Made learning functions easy + nice"" üëçüòé",easy + nice,,,
84,"@TheLingSpace the Gick, Derrick & Wilson book, I'm guessing? üòù",,,y,
